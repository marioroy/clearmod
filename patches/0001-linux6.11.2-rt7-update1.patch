
Remove softirq_preempt, inspired by patch-6.12-rc1-rt1-rt2.patch at
https://cdn.kernel.org/pub/linux/kernel/projects/rt/6.12/incr/


diff -uarp a/include/linux/bottom_half.h b/include/linux/bottom_half.h
--- a/include/linux/bottom_half.h
+++ b/include/linux/bottom_half.h
@@ -35,10 +35,8 @@ static inline void local_bh_enable(void)
 
 #ifdef CONFIG_PREEMPT_RT
 extern bool local_bh_blocked(void);
-extern void softirq_preempt(void);
 #else
 static inline bool local_bh_blocked(void) { return false; }
-static inline void softirq_preempt(void) { }
 #endif
 
 #endif /* _LINUX_BH_H */
diff -uarp a/include/linux/sched.h b/include/linux/sched.h
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1834,7 +1834,6 @@ static inline int dl_task_check_affinity
 }
 #endif
 
-extern bool task_is_pi_boosted(const struct task_struct *p);
 extern int yield_to(struct task_struct *p, bool preempt);
 extern void set_user_nice(struct task_struct *p, long nice);
 extern int task_prio(const struct task_struct *p);
diff -uarp a/kernel/sched/core.c b/kernel/sched/core.c
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -7391,21 +7391,6 @@ static inline void preempt_dynamic_init(
 
 #endif /* CONFIG_PREEMPT_DYNAMIC */
 
-/*
- * task_is_pi_boosted - Check if task has been PI boosted.
- * @p:	Task to check.
- *
- * Return true if task is subject to priority inheritance.
- */
-bool task_is_pi_boosted(const struct task_struct *p)
-{
-	int prio = p->prio;
-
-	if (!rt_prio(prio))
-		return false;
-	return prio != p->normal_prio;
-}
-
 int io_schedule_prepare(void)
 {
 	int old_iowait = current->in_iowait;
diff -uarp a/kernel/sched/rt.c b/kernel/sched/rt.c
--- a/kernel/sched/rt.c
+++ b/kernel/sched/rt.c
@@ -2193,11 +2193,8 @@ static int rto_next_cpu(struct root_doma
 
 		rd->rto_cpu = cpu;
 
-		if (cpu < nr_cpu_ids) {
-			if (!has_pushable_tasks(cpu_rq(cpu)))
-				continue;
+		if (cpu < nr_cpu_ids)
 			return cpu;
-		}
 
 		rd->rto_cpu = -1;
 
diff -uarp a/kernel/softirq.c b/kernel/softirq.c
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -248,19 +248,6 @@ out:
 }
 EXPORT_SYMBOL(__local_bh_enable_ip);
 
-void softirq_preempt(void)
-{
-	if (WARN_ON_ONCE(!preemptible()))
-		return;
-
-	if (WARN_ON_ONCE(__this_cpu_read(softirq_ctrl.cnt) != SOFTIRQ_OFFSET))
-		return;
-
-	__local_bh_enable(SOFTIRQ_OFFSET, true);
-	/* preemption point */
-	__local_bh_disable_ip(_RET_IP_, SOFTIRQ_OFFSET);
-}
-
 /*
  * Invoked from ksoftirqd_run() outside of the interrupt disabled section
  * to acquire the per CPU local lock for reentrancy protection.
diff -uarp a/kernel/time/timer.c b/kernel/time/timer.c
--- a/kernel/time/timer.c
+++ b/kernel/time/timer.c
@@ -1564,16 +1564,9 @@ static void timer_sync_wait_running(stru
 	__releases(&base->lock) __releases(&base->expiry_lock)
 	__acquires(&base->expiry_lock) __acquires(&base->lock)
 {
-	bool need_preempt;
-
-	need_preempt = task_is_pi_boosted(current);
-	if (need_preempt || atomic_read(&base->timer_waiters)) {
+	if (atomic_read(&base->timer_waiters)) {
 		raw_spin_unlock_irq(&base->lock);
 		spin_unlock(&base->expiry_lock);
-
-		if (need_preempt)
-			softirq_preempt();
-
 		spin_lock(&base->expiry_lock);
 		raw_spin_lock_irq(&base->lock);
 	}
